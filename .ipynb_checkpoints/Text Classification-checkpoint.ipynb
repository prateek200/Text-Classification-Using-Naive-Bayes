{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "# importing modules that are required in this project\n",
    "import os\n",
    "from sklearn.model_selection import train_test_split\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "import string\n",
    "from math import log\n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn.naive_bayes import MultinomialNB"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "x=[] # storing document in this x array   \n",
    "y=[] # storing category of document in this y array   \n",
    "\n",
    "for category in os.listdir(\"C:\\\\Users\\\\SONY\\\\Downloads\\\\20_newsgroups\"):\n",
    "    for document in os.listdir(\"C:\\\\Users\\\\SONY\\\\Downloads\\\\20_newsgroups\\\\\"+category):\n",
    "        with open(\"C:\\\\Users\\\\SONY\\\\Downloads\\\\20_newsgroups\\\\\"+category+'\\\\'+document, \"r\") as f:\n",
    "            x.append(f.read())\n",
    "            y.append(category)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "# this function is required to extract feature for dataset\n",
    "\n",
    "def feature_extraction(data):\n",
    "    \n",
    "    dictionary={}\n",
    "    stop_words= list( set(stopwords.words('english')) ) + list(string.punctuation)\n",
    "    \n",
    "    for row in data:\n",
    "        \n",
    "        words=nltk.word_tokenize(row) \n",
    "        \n",
    "        for word in words:\n",
    "            \n",
    "            word=word.lower()\n",
    "            \n",
    "            if(word in stop_words):\n",
    "                continue\n",
    "                \n",
    "            if(word in dictionary):\n",
    "                \n",
    "                dictionary[word]=dictionary[word]+1\n",
    "            \n",
    "            else:\n",
    "                \n",
    "                dictionary[word]=1\n",
    "    \n",
    "    a=[]\n",
    "    \n",
    "    for feature in dictionary:\n",
    "        a.append((dictionary[feature],feature))\n",
    "    \n",
    "    a.sort(reverse=True)\n",
    "    \n",
    "    \n",
    "    Features=[]\n",
    "    \n",
    "    for i in range(2000):\n",
    "        \n",
    "        Features.append(a[i][1])\n",
    "    \n",
    "    return Features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "# this is self implemented multinomial naive bayes fit function\n",
    "# it returns the dictionary\n",
    "\n",
    "def fit(x,y,features):\n",
    "    \n",
    "    dictionary={}\n",
    "    dictionary[\"total_rows\"]=len(y)\n",
    "    classes=set(y)\n",
    "    \n",
    "    for target_class in classes:\n",
    "        \n",
    "        dictionary[target_class]={}\n",
    "        dictionary[target_class][\"total_rows\"]=y.count(target_class)\n",
    "        dictionary[target_class][\"total_words\"]=0\n",
    "        \n",
    "        for feature in features:\n",
    "            \n",
    "            dictionary[target_class][feature]=0\n",
    "        \n",
    "    \n",
    "    for row_number in range(len(x)):\n",
    "        \n",
    "        \n",
    "        words=nltk.word_tokenize(x[row_number])\n",
    "        current_class=y[row_number]\n",
    "        \n",
    "        for word in words:\n",
    "            \n",
    "            word=word.lower()\n",
    "            \n",
    "            if(word not in features):\n",
    "                continue\n",
    "            \n",
    "            dictionary[current_class][word]=dictionary[current_class][word]+1 \n",
    "            dictionary[current_class][\"total_words\"]=dictionary[current_class][\"total_words\"]+1\n",
    "    \n",
    "    \n",
    "    return dictionary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "# this function finds the probablity that words belongs to the target class\n",
    "# this function also uses laplace correction\n",
    "\n",
    "def probability(words,target_class,dictionary):\n",
    "    \n",
    "    prob=log(dictionary[target_class][\"total_rows\"]/dictionary[\"total_rows\"])\n",
    "    \n",
    "    for word in words:\n",
    "        \n",
    "        word=word.lower()\n",
    "        \n",
    "        if(word not in dictionary[target_class]):\n",
    "            continue\n",
    "        \n",
    "        num=dictionary[target_class][word]+1\n",
    "        den=dictionary[target_class][\"total_words\"]+len(dictionary[target_class].keys())-2\n",
    "        \n",
    "        prob=prob+log(num/den)\n",
    "    \n",
    "    return prob    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "# this function return the prediction for testing data \n",
    "\n",
    "def predict(x,dictionary):\n",
    "    \n",
    "    predictions=[]\n",
    "    \n",
    "    for row in x:\n",
    "        \n",
    "        words=nltk.word_tokenize(row)\n",
    "        best_probability=-1000\n",
    "        best_class=\"\"\n",
    "        oneloop=True\n",
    "        \n",
    "        for target_class in dictionary.keys():\n",
    "            \n",
    "            if(target_class==\"total_rows\"):\n",
    "                continue\n",
    "            \n",
    "            prob=probability(words,target_class,dictionary)\n",
    "            \n",
    "            if(oneloop==True or prob>best_probability):\n",
    "                best_probability=prob\n",
    "                best_class=target_class\n",
    "            \n",
    "            oneloop=False\n",
    "            \n",
    "        predictions.append(best_class)\n",
    "    \n",
    "    return predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "# this function create 2D dataset for sklearn implemented multinomial naive bayes\n",
    "\n",
    "def create_dataset(x,features):\n",
    "    \n",
    "    dataset=[[ 0 for __ in range( len(features) ) ] for _ in range( len(x) ) ]\n",
    "    \n",
    "    for row_num in range(len(x)):\n",
    "        \n",
    "        words=nltk.word_tokenize(x[ row_num ]) \n",
    "        \n",
    "        for word in words:\n",
    "            \n",
    "            word=word.lower()\n",
    "            \n",
    "            if(word not in features):\n",
    "                continue \n",
    "            \n",
    "            position=features.index(word)\n",
    "            \n",
    "            dataset[row_num][position]=dataset[row_num][position]+1\n",
    "        \n",
    "    \n",
    "    return dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "# splitting the dataset \n",
    "\n",
    "x_train,x_test,y_train,y_test=train_test_split(x,y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "# extracting features\n",
    "\n",
    "features=feature_extraction(x_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                          precision    recall  f1-score   support\n",
      "\n",
      "             alt.atheism       0.71      0.75      0.73       256\n",
      "           comp.graphics       0.69      0.83      0.75       230\n",
      " comp.os.ms-windows.misc       0.62      0.04      0.08       233\n",
      "comp.sys.ibm.pc.hardware       0.60      0.81      0.69       276\n",
      "   comp.sys.mac.hardware       0.76      0.90      0.83       259\n",
      "          comp.windows.x       0.72      0.72      0.72       246\n",
      "            misc.forsale       0.78      0.89      0.83       263\n",
      "               rec.autos       0.80      0.88      0.84       256\n",
      "         rec.motorcycles       0.82      0.94      0.88       239\n",
      "      rec.sport.baseball       0.86      0.92      0.89       222\n",
      "        rec.sport.hockey       0.96      0.82      0.89       239\n",
      "               sci.crypt       0.94      0.91      0.92       277\n",
      "         sci.electronics       0.78      0.88      0.83       236\n",
      "                 sci.med       0.95      0.84      0.89       250\n",
      "               sci.space       0.95      0.91      0.93       239\n",
      "  soc.religion.christian       0.93      0.99      0.96       253\n",
      "      talk.politics.guns       0.73      0.81      0.76       238\n",
      "   talk.politics.mideast       0.88      0.88      0.88       250\n",
      "      talk.politics.misc       0.76      0.57      0.65       270\n",
      "      talk.religion.misc       0.58      0.54      0.56       268\n",
      "\n",
      "                accuracy                           0.79      5000\n",
      "               macro avg       0.79      0.79      0.78      5000\n",
      "            weighted avg       0.79      0.79      0.78      5000\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# creating dataset and finding classification report for sklearn implemented naive bayes\n",
    "\n",
    "X_train=create_dataset(x_train,features)\n",
    "X_test=create_dataset(x_test,features)\n",
    "\n",
    "clf = MultinomialNB()\n",
    "\n",
    "clf.fit(X_train,y_train)\n",
    "\n",
    "Predictions=clf.predict(X_test)\n",
    "\n",
    "print(classification_report(y_test,Predictions))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                          precision    recall  f1-score   support\n",
      "\n",
      "             alt.atheism       0.71      0.75      0.73       256\n",
      "           comp.graphics       0.69      0.83      0.75       230\n",
      " comp.os.ms-windows.misc       0.62      0.04      0.08       233\n",
      "comp.sys.ibm.pc.hardware       0.60      0.81      0.69       276\n",
      "   comp.sys.mac.hardware       0.76      0.90      0.83       259\n",
      "          comp.windows.x       0.72      0.72      0.72       246\n",
      "            misc.forsale       0.78      0.89      0.83       263\n",
      "               rec.autos       0.80      0.88      0.84       256\n",
      "         rec.motorcycles       0.82      0.94      0.88       239\n",
      "      rec.sport.baseball       0.86      0.92      0.89       222\n",
      "        rec.sport.hockey       0.96      0.82      0.89       239\n",
      "               sci.crypt       0.94      0.91      0.92       277\n",
      "         sci.electronics       0.78      0.88      0.83       236\n",
      "                 sci.med       0.95      0.84      0.89       250\n",
      "               sci.space       0.95      0.91      0.93       239\n",
      "  soc.religion.christian       0.93      0.99      0.96       253\n",
      "      talk.politics.guns       0.73      0.81      0.76       238\n",
      "   talk.politics.mideast       0.88      0.88      0.88       250\n",
      "      talk.politics.misc       0.76      0.57      0.65       270\n",
      "      talk.religion.misc       0.58      0.54      0.56       268\n",
      "\n",
      "                accuracy                           0.79      5000\n",
      "               macro avg       0.79      0.79      0.78      5000\n",
      "            weighted avg       0.79      0.79      0.78      5000\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# this is self implemented multinomial naive \n",
    "# fitting and prediction for test data and comparing the classification report with sklearn \n",
    "# implemented\n",
    "\n",
    "dictionary=fit(x_train,y_train,features)\n",
    "\n",
    "predictions=predict(x_test,dictionary)\n",
    "\n",
    "print(classification_report(y_test,predictions))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# this is the function which validates the word \n",
    "# but this project doesnot use this beacause time will increase \n",
    "# it increases the accuracy to 83 %\n",
    "\n",
    "def validate_word(word):\n",
    "    \n",
    "    punctuation=string.punctuation+\"1234567890\"\n",
    "    \n",
    "    new_word=\"\"\n",
    "    a=[]\n",
    "    \n",
    "    for x in range(len(word)):\n",
    "        \n",
    "        if(word[x] in punctuation):\n",
    "            a.append(x)\n",
    "    \n",
    "    if(len(a)>2):\n",
    "        return \"\"\n",
    "    \n",
    "    for x in range(len(word)):\n",
    "        \n",
    "        if(x not in a):\n",
    "            new_word+=word[x]\n",
    "    \n",
    "    return new_wordz"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
